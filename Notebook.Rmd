---
title: "Statistical Methods in Data Science"
subtitle : "Homework 2" 
author: "Aguanno Irene - Barboni Alessio"
date: "A.Y. 2021-2022"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
---


In game theory, the **Shapley value** is a rule useful for fairly dividing up the productive value generated by some set of individuals (a *coalition*) among its members. It is based on Lloyd Shapley's idea that the members of a coalition should receive a payment that is proportional to their marginal contribution.


# Exercises

## 1. Introductory

First and foremost, we find the Shapley values for the three players of a simple game that requests them to collaborate in order to save working hours. The players, which we call **A**, **B** and **C**, form a coalition *C* such that ν(**A**)=ν(**B**)=ν(**C**)= 0 and ν(**ABC**)= 4, with ν(*C*)={# of hours potentially saved by well organized coalition}.

We end up with the following table showing the value brought to the coalition by each player while the *grand coalition* (i.e., the coalition formed by all players) gets formed.

```{r, echo=FALSE, warning=FALSE}
library(kableExtra)

text_tbl <- data.frame(
  Permutations = c("ABC", "ACB", "BAC", "BCA", "CAB", "CBA"),
  Player_A = c(0, 0, 2, 2, 0, 2),
  Player_B = c(2, 4, 0, 0, 4, 2),
  Player_C = c(2, 0, 2, 2, 0, 0)
)

kable(text_tbl) %>%
  kable_styling(full_width = F)
```

According to the table, then, we obtain the following Shapley values:

$\psi (A) = 6/6 = 1$

$\psi (B) = 12/6 = 2$

$\psi (C) = 6/6 = 1$.

Such results seem reasonable, since **B** works during the hours in the middle of the day and can therefore cover more shifts. We also check that $\psi(A)+\psi(B)+\psi(C)=v(ABC)= 4$.


## 2.Probabilistic

Now we have many more players to handle, but the procedure remains essentially the same. The Shapley Value is still calculated as the mean of differences between the value of the characteristic function of a coalition containing a certain player X, and the value for the same coalition but without player X (i.e., it aims at measuring the contribution of a single player, X, to that specific coalition). In the shapley table, the coalition members depend on the permutations of the players and on the player itself for whom we are calculating the Shapley Value. The coalitions that form for the X-th column are the result of slicing every permutation array up to element X, that is then splitted as described above.

Importing libraries and the dataset:
```{r, warning=FALSE, message=FALSE} 
library(combinat)
load("C:/Users/alema/Downloads/char_fun.RData")

```

Initializing variables:
```{r, warning=FALSE, message=FALSE} 
p <- 12
myPlayers = c(1,2,3) # chosen arbitrarily 
eps <- 0.01
alpha <- 0.05
gamma <- 1-alpha 
```
We chose a 95% confidence level, and an epsilon of 1%.

Approximating the boundaries $(a, b)$ for Shapley Values:

The tighter the boundaries, the smaller the sample size needed for a good enough approximation would be. Here also we want to find an approximation for $a$ and $b$, since finding the real boundaries would be at least as costly as finding the shapley value itself. Of course, these bounds must be inclusive, i.e., they must contain every possible value of the shapley value domain (otherwise the sample size would be too small, and Hoeffding approximation would have no guarantee to hold). For this reason we have excluded those greedy approaches that try converging to a local minimum/maximum, given that there is not guarantee that they would end up containing every possible element. 

To avoid this problem, we opted for finding the smallest/largest difference for each coalition size pair (i.e., (size=2,size=1), ..., (size=12,size=11)), in this way the bounds would surely contain each possible value and would probably end up being even looser than needed. Given that there are 12 positions that player $X$ can take in the array, for each of those, associated with a different extremum, there are $11 \choose i$ different entries in the shapley table, were $i$ represents the number of players in the smaller coalition (since the #possibilities depends on the #combinations of the 11 other players over the i-1 array places).

```{r, warning=FALSE, message=FALSE} 
a<-0
for (i in (2:(p-1))){
  nChooseK <- prod(c((p-1):c(p-i-1+1, i-1+2)[match(max(p-1-i, i), c(p-i-1, i))] ))/factorial(min(p-i-1, i))
  if (i==(p-1)) nChooseK=1
  a<-a+(max(0, min(char_fun[[i]]) - max(char_fun[[(i-1)]])))*nChooseK 
}
a<-floor(a/(2**(p-1)-1))
sprintf("The lower bound approximation, a, for Shapley Values is: %s", a)
```

```{r, warning=FALSE, message=FALSE}
b<-0
for (i in (2:(p-1))){
  nChooseK <- prod(c((p-1):c(p-i-1+1, i-1+2)[match(max(p-1-i, i), c(p-i-1, i))] ))/factorial(min(p-i-1, i))
  if (i==(p-1)) nChooseK=1
  b<-b+(max(char_fun[[i]]) - min(char_fun[[(i-1)]]))*nChooseK 
}
b<-ceiling(b/(2**(p-1)-1))
sprintf("The upper bound approximation, b, for Shapley Values is: %s", b)
```

Calculating the sample size needed for Hoeffding approximation:
```{r, warning=FALSE, message=FALSE} 
M_star <- ceiling(((b - a)**2)/(2 * eps**2) * log(2/(alpha)))
sprintf("The required sample size for our approximation is: %s", M_star)
```

Compute the first N permutations of $p!$:

Since there are 12 different players, its factorial would be a really large number (i.e., $4.79e+08$). Hence, for efficiency reasons, instead of computing first all the permutations of the 12 players and then take only the first N (or better, M_star that is $3.61e+6$), we can simply stop the algorithm as soon as it computes the first M_star permutations. This is true because we need just one estimate for each one of the three players, if we needed more we would have needed all the permutations instead, plus the function "sample" for shuffling them to get different M_star samples at each iteration. Anyway, in order to calculate the first N permutations, we decided to modify the "permn" function from $R$'s "combinat" library. The whole function is the one below, the changes that we made are the ones commented.


```{r, warning=FALSE, message=FALSE, cache=TRUE} 
firstNPerm <- function (x, fun = NULL, ...) #Takes the first N permutations of p!
{
  if (is.numeric(x) && length(x) == 1 && x > 0 && trunc(x) == 
      x) 
    x <- seq(x)
  n <- length(x) 
  nofun <- is.null(fun)
  out <- vector("list", M_star) #gamma(n+1) changed into M_star, i.e. the desired output size
  p <- ip <- seqn <- 1:n
  d <- rep(-1, n)
  d[1] <- 0
  m <- n + 1
  p <- c(m, p, m)
  i <- 1
  use <- -c(1, n + 2)
  clt<-0 # added a control variable to end the loop after M_star iterations
  while (m != 1) {
    if (clt==M_star) break # added a control condition to end the loop
    clt<-clt+1 # added an expression to increase the iterations counter
    out[[i]] <- if (nofun) 
      x[p[use]]
    else fun(x[p[use]], ...)
    i <- i + 1
    m <- n
    chk <- (p[ip + d + 1] > seqn)
    m <- max(seqn[!chk])
    if (m < n) 
      d[(m + 1):n] <- -d[(m + 1):n]
    index1 <- ip[m] + 1
    index2 <- p[index1] <- p[index1 + d[m]]
    p[index1 + d[m]] <- m
    tmp <- ip[index2]
    ip[index2] <- ip[m]
    ip[m] <- tmp
  }
  out[1:M_star]
}
permutations <- firstNPerm(c(1:12))
```


### Hoeffding Estimate of Shapley Values:

Here we calculate the shapley values of the first three players, that we arbitrarily chose.  
```{r, warning=FALSE, message=FALSE, cache=TRUE} 
I <- c(0,0,0)
for (i in (1:M_star)){
  iPerm <- permutations[i][[1]]
  for (player in myPlayers){
    if (length(iPerm[1:(match(player, iPerm))])==1) next
    key <-sort(iPerm[1:(match(player, iPerm)-1)])
    key1 <- ""
    for (i in (1:length(key))){
      key1<-paste(key1,"-",key[i],sep="")
    }
    key1<-substr(key1, 2, nchar(key1))
    
    key <-sort(iPerm[1:(match(player, iPerm))])
    key2 <- ""
    for (i in (1:length(key))){
      key2<-paste(key2,"-",key[i],sep="")
    }
    key2<-substr(key2, 2, nchar(key2))
    I[player] <- I[player] + (char_fun[[length(key)]][[key2]] - char_fun[[(length(key)-1)]][[substr(key1, 0, nchar(key1))]])
  }
}
cat("Shapley Values Approximations:", "\n","Player 1:", I[1]/M_star,"\n","Player 2:", I[2]/M_star,"\n","Player 3:", I[3]/M_star)
```
As we can see above, Player 1's shapley value is quite smaller with respect to the one of the other two. A reasonable explaination might be related to the players' working hours. Player 1 is able to work from 12.00 to 17.00 non-stop, but cannot from 9.00 to 12.00. The latter time period is much more requested in some sense by the possible coalitions that may form, since there are 16 other holes from other players' working schedule. Player 2 instead has the highest shapley value of the three, its only flaw that is penalizing him probably is the 16.00-17.00 hole. Player 3 is in the middle, as one could have expected.  

### Hoeffding Confidence Intervals:
```{r, warning=FALSE, message=FALSE} 
I<-c(1.726239, 6.126664, 6)
t_alpha <- sqrt((b-a)**2/(2*M_star) * log(2/alpha)) 
i<-1
for (shap in I){
  print(eval(sprintf("Player %s confidence interval is: (%s,%s)",i,round(shap-t_alpha,2),round(shap+t_alpha,2))))
  i<-i+1
}

```

### Bonus Point 
The code below aims at building the characteristic function for a generic game of this type. To test if it is working we will use the provided working schedule that we have rewritten in a csv file.

```{r, warning=FALSE, message=FALSE, cache=TRUE}
library(stringr)
df <- read.csv("bonus.csv", sep=";")
df <- df[-c(13:15),-c(1,10:14)]
df
```
Besides some string manipulation for creating the keys for the characteristic function, this function computes its values for a certain combination considering "Yes" and "No" as 1s and 0s, to be able to easily take the row sum between players in each coalition. Whenever the resulting row/array has elements larger than 1 it means that some hours have been saved. Hence, it computes the sum of these larger-than-one values and then subtracts the number of summed elements (since in the end a player of the coalition must do the work in each of these time slots). 

```{r, warning=FALSE, message=FALSE, cache=TRUE}
charFunCalculator <- function(timeSchedule){
numPlayers <- nrow(df)
numHours <- ncol(df) 
colnames(df)<-NULL
rownames(df)<-NULL
charFun <- list()
for (i in (1:numPlayers)){
  combs <- combn((1:numPlayers), i)
  if (i==numPlayers) combs<-matrix(combn((1:numPlayers), numPlayers), ncol = 1)
  tempNames <- c() 
  tempValues <- c()
  for (j in (1:ncol(combs))){
    key <- ""
    coalition <- combs[,j]
    hoursSaved <-c(rep(0, numHours))
    for (player in coalition){
      key<-paste(key, player,"-", sep="")
      df[player,1:numHours] <- str_replace_all(df[player,1:numHours], "Yes", "1")
      df[player,1:numHours] <- strtoi(str_replace_all(df[player,1:numHours], "No", "0"))
      hoursSaved <- hoursSaved+as.numeric(df[player,1:numHours])
    }
    key <- substr(key, 1, nchar(key)-1)
    tempNames <- c(tempNames, key)
    tempValues <- c(tempValues, sum(hoursSaved[hoursSaved>1])-length((hoursSaved[hoursSaved>1])))
  }
  names(tempValues) <- tempNames
  charFun <- append(charFun, list(tempValues))
}
return(charFun)
}
charFun <- charFunCalculator(df)
```
Let's check now if it is equal to the char_fun that was provided by the homework.
```{r, warning=FALSE, message=FALSE, cache=TRUE}
for (i in (1:12)){
  print(all(charFun[[i]]==char_fun[[i]]))
}
```
## 3. Statistical

For this task, our aim is to build a portfolio of stocks and to find out the Shapley value associated to each share. More precisely, we want to know how much each asset in the portfolio contributes to the total utility $U_{\omega}$ received by the financial investor owning the portfolio. Note that, in our case, the utility function  $U_{\omega}$ is a linear combination of the portfolio average return and volatility:
$U_{\omega} = E(X) − \omega · Var(X)$ (*Eq. 1*) for some weight $\omega > 0$.

```{r, warning=FALSE, message=FALSE}
library(readxl)
library(quantmod)
library(confintr)
library(data.table)
library(igraph)
library(GGally)
library(boot)
```


### 3.1

First of all, we create our portfolio. We do so by collecting assets from different GICS sectors. Our expectation is that the marginal correlation graphs will show stocks from the same sectors clustering together, as they supposedly interact more with each other.

As can be seen in the table below, we select 16 stocks from the Energy sector, 13 from Finance, 12 from Public Utilities, 10 from Real Estate, and 6 from Technology, for a total of 53 assets.
```{r, cache = TRUE, warning=FALSE, results='hide', message=FALSE}
data_env <- new.env()
stocks <- read_excel("aziende.xlsx")

# Use getSymbols to load data into the environment
today <- Sys.Date()
getSymbols(Symbols = stocks$Symbol,src = "yahoo", from = "2020-01-01", to = today, 
           frequency = "daily", env = data_env, auto.assign = TRUE)
```

```{r, echo=FALSE}
kable(stocks)
```


For each stock, we download the daily closing prices (adjusted by dividends) from January 1st, 2020 until today (December 24th, 2021). This results in exactly 500 observations for each stock. Finally, we compute a matrix $X$ whose entries are calculated as $x_{t,j} = log(c_{t,j}/c_{t−1,j})$ (*Eq. 2*), with $c_{t,j}$ being the closing price of stock $j$ on day $t$. $X$ will be the starting point of our analysis.
```{r, cache = TRUE, warning=FALSE}
# Extract closing price column from each object
close_list <- lapply(data_env, Cl)
# Merge each list element into one object
close <- do.call(merge, close_list)
head(close)
```

```{r, cache = TRUE, warning=FALSE}
# take the difference between log(x_j,t) and log(x_j-1, t)
X <- diff(log(close))
# remove first row since NAs are generated
X <- X[-1]
# transform to dataframe
X <- as.data.frame(X)
# order columns by name
X <-X[,order(names(X) )]
stocks$Symbol <- colnames(X)
# remove .Close from name
stocks$Symbol <-  gsub(".Close", "", stocks$Symbol)
```


### 3.2
We now estimate the Pearson-based marginal correlation graph over stocks, starting from $X$. To this end, we first compute asymptotic Normal-based confidence intervals (with alpha left to the default value of 5%) between all possible pairs of stocks in our portfolio and - when the result obtained is statistically significant (i.e., the confidence interval does not contain 0) - we store it in a new matrix.
```{r, cache = TRUE, warning=FALSE}
# initialize matrix that will contain significant Pearson correlation coefficients
Pearsons <- matrix(NA,ncol=length(stocks$Symbol),nrow=length(stocks$Symbol))
rownames(Pearsons) <- stocks$Symbol
colnames(Pearsons) <- stocks$Symbol

# for each pair of stocks i, j
for(i in 1:length(stocks$Symbol)){for(j in i:length(stocks$Symbol)){
  # if the confidence interval for the pearson coefficient does not contain the zero
  if (!(ci_cor(X[,c(i,j)])$interval[1] < 0 & ci_cor(X[,c(i,j)])$interval[2]>0)) {
    # add the coefficient to the (i,j)-th position in the matrix
    Pearsons[i,j] <- cor(X[,i], X[,j])
  }
}}      
```

```{r, cache = TRUE, warning=FALSE}
Pearsons <- data.frame(Pearsons)
```


### 3.3
Now that we have the theoretical procedure to build our Pearson-based confidence intervals, it’s time to visualize some results. The marginal correlation graph is created by putting an edge between stocks $j$ and $k$ when $|\rho(j,k)|≥ \epsilon$, with   $\rho$ being the Pearson correlation coefficient obtained earlier and $\epsilon$ a threshold between 0 and 1.
```{r, cache = TRUE, warning=FALSE, echo=FALSE}
# assign colors to the stocks for visualization purposes
color_sector <- function(x, df = stocks){
  color <- stocks[which(stocks$Symbol == x),]
  return(color$Sector)
}
sectors <- sapply(colnames(df), color_sector)
```

```{r, cache = TRUE, warning=FALSE, echo=FALSE}
# function to plot the graphs
graph_plot <- function(e){
   ### param e: threshold 
   ### returns: marginal correlation graph
  
  # create adjacency matrix 
  df <- apply(Pearsons, 2, function(x) ifelse(x > e, 1, 0))
  df[is.na(df)] <- 0
  
  color_sector <- function(x, df = stocks){
  color <- stocks[which(stocks$Symbol == x),]
  return(color$Sector)
  }
  
sectors <- sapply(colnames(df),color_sector)

# plot
ggnet2(df, color = sectors, palette = "Set1", node.size = 4, label = TRUE, label.size = 3) + ggtitle(paste("Marginal Correlation Graph with epsilon =", e))
}
```


```{r, cache = TRUE, warning=FALSE,  out.width="80%", out.height="80%", echo=FALSE}
graph_plot(0.3)
graph_plot(0.5)
graph_plot(0.6)
graph_plot(0.7)
graph_plot(0.8)
graph_plot(0.9)
```

Cool! As expected, stocks from companies belonging to the same GICS sector tend to have higher correlations and thus appear clustered together, especially for $\epsilon>0.5$.

A few nodes seem to be out of place, like AMT (which belongs to the Finance sector, but is linked to two companies from the Real Estate sector) in the epsilon = 0.7 graph or EPD (from Public Utilities, but connected to companies of the Energy field) in the same graph. Still, upon further inspection, we discover that AMT (American Tower Corporation) actually is ["one of the largest global Real Estate Investment Trusts (REITs)"](https://www.americantower.com/company/) and EPD (Enterprise Products Partners) is ["one of the largest Midstream Oil and Gas companies in North America"](https://www.enterpriseproducts.com/), so it makes sense for them to be clustered together with Real Estate and Energy companies, respectively.

It is also nice to observe that with $\epsilon>0.8$ only stocks of the same sector are connected.



### 3.4

Now that we gained insight on how our stocks are correlated, we proceed with the estimation of the Shapley values $\psi(j)$ for each stock $j$. Since computing the Shapley attribution may be prohibitive when the number of players (here, stocks) is large, we approximate them via a bootstrap simulation.

The formula used to obtain such estimate is the following:
$\psi(j)  = E(X_j ) − \omega · \displaystyle\sum_{r=1}^{p} Cov(X_j , X_r)$ (*Eq. 3*).

We experiment with two different values for $\omega$ (a smaller one, 0.1, and a larger one, 5) and two types of confidence intervals (pivotal and normal).
```{r, cache = TRUE, warning=FALSE}
# apply formula for the Shapley values
shapley <- function(x, omega = 1){
  ### param x: matrix with the data
  ### param omega: positive weight
  
  # compute expected value of each stock
  ex <- apply(x, 2, mean)
  covariance = cov(x)
  # sum the covariances
  c <- apply(covariance, 1, sum)
  # apply formula
  sh = ex + omega * c
  return(sh)
}
```

```{r, cache = TRUE, warning=FALSE}
# Estimator
shap <- shapley(X, omega = 0.1)
```

```{r, cache = TRUE, warning=FALSE}
set.seed(42)
# number of bootstrap iterations
B      <- 1000
# initialize matrix containing bootstrpped replicates
shboot <- matrix(NA, ncol = dim(X)[2], nrow = B)
colnames(shboot) <- stocks$Symbol

# Main loop
for (b in 1:B){
    idx = sample(1:dim(X)[1], dim(X)[1], 
               replace = TRUE)
    xboot = X[idx,]
    shboot[b,] = shapley(xboot, omega = 0.1)
}
```

```{r, cache = TRUE, warning=FALSE, results='hide'}
# Pivotal
alpha <- 0.05
# compute pivotal confidence intervals
interv <- matrix(NA, nrow = 2, ncol = length(stocks$Symbol))
interv[1,] <- 2*shap - quantile(shboot, 1 - alpha/2)
interv[2,] <- 2*shap - quantile(shboot, alpha/2)

colnames(interv) <- stocks$Symbol
rownames(interv) <- c("Lower","Upper")
```


```{r, cache = TRUE, warning=FALSE, out.width="100%", out.height="80%", echo=FALSE}
# plot the intervals
df <- data.frame(x = stocks$Symbol,
                 F =shap,
                 L =interv[1,],
                 U =interv[2,])

require(ggplot2)
ggplot(df, aes(x = x, y = F)) +
  geom_point(size = 4) +
  geom_errorbar(aes(ymax = U, ymin = L))+
  ggthemes::theme_economist() +  ggtitle("Shapley values Pivotal C.I. with omega = 0.1") + 
  
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        axis.title.x = element_blank(),
        axis.title.y = element_blank())
```


```{r, cache = TRUE, echo=FALSE, out.width="100%", out.height="80%"}
shap <- shapley(X, omega = 5)

set.seed(42)
B      <- 1000
shboot <- matrix(NA, ncol = dim(X)[2], nrow = B)
colnames(shboot) <- stocks$Symbol

# Main loop
for (b in 1:B){
    idx = sample(1:dim(X)[1], dim(X)[1], 
               replace = TRUE)
    xboot = X[idx,]
    shboot[b,] = shapley(xboot, omega = 5)
}

# Pivotal
alpha <- 0.05
interv <- matrix(NA, nrow = 2, ncol = length(stocks$Symbol))
interv[1,] <- 2*shap - quantile(shboot, 1 - alpha/2)
interv[2,] <- 2*shap - quantile(shboot, alpha/2)



colnames(interv) <- stocks$Symbol
rownames(interv) <- c("Lower","Upper")

df <- data.frame(x = stocks$Symbol,
                 F =shap,
                 L =interv[1,],
                 U =interv[2,])

require(ggplot2)
ggplot(df, aes(x = x, y = F)) +
  geom_point(size = 4) +
  geom_errorbar(aes(ymax = U, ymin = L))+
  ggthemes::theme_economist() +  ggtitle("Shapley values Pivotal C.I. with omega = 5") +
  
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        axis.title.x = element_blank(),
        axis.title.y = element_blank())
```

From the two graphs above, what really strikes the eye is the difference in scale. By increasing $\omega$, we get much larger confidence intervals. This is reasonable since - as can be seen in Eq. 1 - $\omega$ amplifies the variability of the stock. Therefore, the width of the confidence intervals reflects the level of risk aversion of the stocks' holder. A larger confidence interval - implying a larger $\omega$ - expresses a greater aversion to risk. 


Another method to construct confidence intervals is with Normal intervals. To build them correctly, one should already have knowledge about the asymptotic Gaussianity of the estimator, otherwise the interval is not accurate. Since in section 3.2 of this report we already made this assumption, we repeat the analysis in this new context.

```{r, cache = TRUE, warning=FALSE, out.width="100%"}
# Normal
shap <- shapley(X, omega = 0.1)

set.seed(42)
B      <- 1000
shboot <- matrix(NA, ncol = dim(X)[2], nrow = B)
colnames(shboot) <- stocks$Symbol

# Main loop
for (b in 1:B){
    idx = sample(1:dim(X)[1], dim(X)[1], 
               replace = TRUE)
    xboot = X[idx,]
    shboot[b,] = shapley(xboot, omega = 0.1)
}

# get standard error
sdt <- apply(shboot,2,sd)
alpha <- 0.05
# compute normal-based confidence intervals
interv <- matrix(NA, nrow = 2, ncol = length(stocks$Symbol))
interv[1,] <- shap - qnorm(1 - alpha/2)*sdt
interv[2,] <- shap - qnorm(alpha/2)*sdt

colnames(interv) <- stocks$Symbol
rownames(interv) <- c("Lower","Upper")

```

```{r, cache = TRUE, warning=FALSE, out.width="100%", echo = FALSE}

df <- data.frame(x = stocks$Symbol,
                 F =shap,
                 L =interv[1,],
                 U =interv[2,])

require(ggplot2)
ggplot(df, aes(x = x, y = F)) +
  geom_point(size = 4) +
  geom_errorbar(aes(ymax = U, ymin = L))+
  ggthemes::theme_economist() + ggtitle("Shapley values Normal C.I. with omega = 0.1") +
  
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        axis.title.x = element_blank(),
        axis.title.y = element_blank())
```

```{r, cache = TRUE, warning=FALSE, out.width="100%", echo = FALSE}
shap <- shapley(X, omega = 5)

set.seed(42)
B      <- 1000
shboot <- matrix(NA, ncol = dim(X)[2], nrow = B)
colnames(shboot) <- stocks$Symbol

# Main loop
for (b in 1:B){
    idx = sample(1:dim(X)[1], dim(X)[1], 
               replace = TRUE)
    xboot = X[idx,]
    shboot[b,] = shapley(xboot, omega = 5)
}

sdt <- apply(shboot,2,sd)
alpha <- 0.05
interv <- matrix(NA, nrow = 2, ncol = length(stocks$Symbol))
interv[1,] <- shap - qnorm(1 - alpha/2)*sdt
interv[2,] <- shap - qnorm(alpha/2)*sdt



colnames(interv) <- stocks$Symbol
rownames(interv) <- c("Lower","Upper")

df <- data.frame(x = stocks$Symbol,
                 F =shap,
                 L =interv[1,],
                 U =interv[2,])

require(ggplot2)
ggplot(df, aes(x = x, y = F)) +
  geom_point(size = 4) +
  geom_errorbar(aes(ymax = U, ymin = L))+
  ggthemes::theme_economist() + ggtitle("Shapley values Normal C.I. with omega = 5") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        axis.title.x = element_blank(),
        axis.title.y = element_blank())
```

Again, we notice the effects that $\omega$ produces on the width of the confidence intervals. Moreover, we observe some differences with respect to the pivotal type of intervals. In particular, in the normal case the intervals appear to be larger in width and higher in value.
